# ğŸš€ ë¡œì»¬ ëŒ€í™” ëª¨ë¸ ë° ë¹ ë¥¸ ëª¨ë¸ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2026-01-20  
**ëª©ì **: ë¡œì»¬ ëŒ€í™” ëª¨ë¸ í™•ì¸ ë° ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸ ì¶”ì²œ  
**ìƒíƒœ**: âœ… ë¶„ì„ ì™„ë£Œ

---

## ğŸ¯ í•µì‹¬ ë‹µë³€

### 1. ë¡œì»¬ ëŒ€í™” ëª¨ë¸ í™•ì¸

**âœ… ë„¤, ë¡œì»¬ ëŒ€í™” ëª¨ë¸ì´ ì´ë¯¸ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤!**

**ìœ„ì¹˜**: `tools/local_gemma3_engine.py`

**ê¸°ëŠ¥**:
- ë¡œì»¬ Ollama ê¸°ë°˜ Gemma3 ì¶”ë¡  ì—”ì§„
- ì»¤ì„œ IDEì™€ ì™„ì „íˆ ë…ë¦½ëœ í™˜ê²½ì—ì„œ ì‘ë™
- 100% ë¡œì»¬ ì§€ëŠ¥ ì£¼ê¶Œ í™•ë³´
- 150ms ì§ê´€ì˜ ë¬¼ë¦¬ì  í•œê³„ ë„ì „ ëª©í‘œ

**í˜„ì¬ ì„¤ì •**:
- ê¸°ë³¸ ëª¨ë¸: `gemma2:2b` (ì†ë„ ìµœì í™”)
- URL: `http://localhost:11434`
- íƒ€ì„ì•„ì›ƒ: 60ì´ˆ

---

### 2. ëŒ€í™”ê°€ ì•ˆ ëŠê¸°ê³  ì œì¼ ë¹ ë¥¸ ëª¨ë¸

**ê¶Œì¥ ëª¨ë¸ ìˆœìœ„** (ì†ë„ + ëŒ€í™” ì§€ì†ì„± ê¸°ì¤€):

#### ğŸ¥‡ 1ìœ„: **llama3.2:3b** â­â­â­â­â­

**ì¥ì **:
- âœ… **ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ ì†ë„** (TTFT: 200-500ms ì˜ˆìƒ)
- âœ… **ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ ìš°ìˆ˜** (Ollama ìë™ ê´€ë¦¬)
- âœ… **í•œêµ­ì–´ ì„±ëŠ¥ ì¤€ìˆ˜**
- âœ… **ê²½ëŸ‰ ëª¨ë¸** (3B íŒŒë¼ë¯¸í„°, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )

**ì„¤ì¹˜**:
```bash
ollama pull llama3.2:3b
```

**ì‚¬ìš©**:
```typescript
// api.tsì—ì„œ ëª¨ë¸ ë³€ê²½
const requestBody: Gemma3Request = {
  model: 'llama3.2:3b',  // gemma3:4b â†’ llama3.2:3b
  prompt: fullPrompt,
  stream: false,
  options: {
    temperature: 0.7,
    num_predict: 500
  }
};
```

---

#### ğŸ¥ˆ 2ìœ„: **gemma2:2b** â­â­â­â­

**ì¥ì **:
- âœ… **ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ** (TTFT: 300-600ms ì˜ˆìƒ)
- âœ… **êµ¬ê¸€ ê²½ëŸ‰ ëª¨ë¸** (ìš”ì•½ ì˜í•¨)
- âœ… **í˜„ì¬ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë¨**

**ë‹¨ì **:
- âš ï¸ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ê°€ llama3.2:3bë³´ë‹¤ ì•½ê°„ ë‚®ìŒ

**ì„¤ì¹˜**:
```bash
ollama pull gemma2:2b
```

---

#### ğŸ¥‰ 3ìœ„: **phi3:mini** â­â­â­â­

**ì¥ì **:
- âœ… **ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ** (TTFT: 250-500ms ì˜ˆìƒ)
- âœ… **Microsoft ê²½ëŸ‰ ëª¨ë¸** (ì½”ë“œ ìƒì„± ìš°ìˆ˜)
- âœ… **ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ ì–‘í˜¸**

**ì„¤ì¹˜**:
```bash
ollama pull phi3:mini
```

---

#### 4ìœ„: **qwen2.5:1.5b** â­â­â­

**ì¥ì **:
- âœ… **ì´ˆê²½ëŸ‰ ëª¨ë¸** (1.5B, ê°€ì¥ ì‘ìŒ)
- âœ… **í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜**
- âœ… **Sub-200ms ë‹¬ì„± ê°€ëŠ¥** (ì–‘ìí™” ì‹œ)

**ë‹¨ì **:
- âš ï¸ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ê°€ í° ëª¨ë¸ë³´ë‹¤ ë‚®ìŒ

**ì„¤ì¹˜**:
```bash
ollama pull qwen2.5:1.5b
```

---

## ğŸ“Š ëª¨ë¸ ì†ë„ ë¹„êµ (ì˜ˆìƒ)

| ëª¨ë¸ | TTFT (ì²« í† í°) | TPS (í† í°/ì´ˆ) | ëŒ€í™” ì§€ì†ì„± | ë©”ëª¨ë¦¬ |
|------|---------------|--------------|------------|--------|
| **llama3.2:3b** | 200-500ms | 50-80 | â­â­â­â­â­ | ~2GB |
| **gemma2:2b** | 300-600ms | 40-70 | â­â­â­â­ | ~1.5GB |
| **phi3:mini** | 250-500ms | 45-75 | â­â­â­â­ | ~2.5GB |
| **qwen2.5:1.5b** | 150-400ms | 60-90 | â­â­â­ | ~1GB |
| **gemma3:4b** (í˜„ì¬) | 500-1000ms | 30-50 | â­â­â­â­ | ~2.5GB |

**ê²°ë¡ **: **llama3.2:3b**ê°€ ì†ë„ì™€ ëŒ€í™” ì§€ì†ì„±ì˜ ìµœì  ê· í˜•ì ì…ë‹ˆë‹¤.

---

## ğŸ”§ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ ë°©ë²•

### Ollama ìë™ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

**OllamaëŠ” ìë™ìœ¼ë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤**:

```typescript
// ì²« ë²ˆì§¸ ì§ˆë¬¸
const response1 = await askGemma3("ì•ˆë…•í•˜ì„¸ìš”", context);

// ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ì´ì „ ëŒ€í™” ìë™ ì°¸ì¡°)
const response2 = await askGemma3("ë‚´ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?", context);
// â†’ "ì•ˆë…•í•˜ì„¸ìš”"ì—ì„œ ì´ë¦„ì„ ë¬¼ì–´ë´¤ìœ¼ë¯€ë¡œ ì´ì „ ëŒ€í™” ì°¸ì¡°
```

**ì£¼ì˜ì‚¬í•­**:
- OllamaëŠ” **ì„¸ì…˜ë³„ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€**í•©ë‹ˆë‹¤
- ê°™ì€ `ollama.chat()` í˜¸ì¶œ ë‚´ì—ì„œë§Œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
- ìƒˆë¡œìš´ ì„¸ì…˜ì´ë©´ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”

---

### ìˆ˜ë™ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ (ê¶Œì¥)

**ë” ì•ˆì •ì ì¸ ëŒ€í™” ì§€ì†ì„ ìœ„í•´ ìˆ˜ë™ ê´€ë¦¬**:

```typescript
// ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
const conversationHistory: Array<{role: string, content: string}> = [];

// ì§ˆë¬¸ ì¶”ê°€
conversationHistory.push({ role: 'user', content: question });

// Ollamaì— ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ë‹¬
const response = await ollama.chat({
  model: 'llama3.2:3b',
  messages: conversationHistory,  // ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ë‹¬
  stream: false
});

// ë‹µë³€ ì¶”ê°€
conversationHistory.push({ role: 'assistant', content: response.message.content });
```

**ì¥ì **:
- âœ… ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì™„ë²½ ìœ ì§€
- âœ… ì„¸ì…˜ ì¢…ë£Œ í›„ì—ë„ ìœ ì§€ ê°€ëŠ¥ (ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì €ì¥)
- âœ… ë” ì •í™•í•œ ëŒ€í™” íë¦„

---

## ğŸš€ mkm-study20260120ì— ì ìš©í•˜ê¸°

### 1. ë¹ ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½

**íŒŒì¼**: `projects/mkm/mkm-study20260120/src/utils/api.ts`

**ìˆ˜ì •**:
```typescript
const requestBody: Gemma3Request = {
  model: 'llama3.2:3b',  // gemma3:4b â†’ llama3.2:3b
  prompt: fullPrompt,
  stream: false,
  options: {
    temperature: 0.7,
    num_predict: 500
  }
};
```

---

### 2. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€

**íŒŒì¼**: `projects/mkm/mkm-study20260120/src/components/MKMStudyApp.tsx`

**ì¶”ê°€**:
```typescript
// ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒíƒœ ì¶”ê°€
const [conversationHistory, setConversationHistory] = useState<Array<{role: string, content: string}>>([]);

// answerQuestion í•¨ìˆ˜ ìˆ˜ì •
const handleSpeechTranscript = async (text: string, isFinal: boolean) => {
  if (isFinal && text.trim()) {
    setQuestion(text.trim());
    setAnswer('');
    setIsListening(false);
    
    // ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì§ˆë¬¸ ì¶”ê°€
    const updatedHistory = [
      ...conversationHistory,
      { role: 'user', content: text.trim() }
    ];
    
    try {
      // ì „ì²´ íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ë‹µë³€ ìš”ì²­
      const response = await answerQuestionWithHistory(
        text.trim(),
        currentState,
        updatedHistory  // ì „ì²´ íˆìŠ¤í† ë¦¬ ì „ë‹¬
      );
      
      // ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë‹µë³€ ì¶”ê°€
      setConversationHistory([
        ...updatedHistory,
        { role: 'assistant', content: response }
      ]);
      
      setAnswer(response);
    } catch (error) {
      console.error('[ë‹µë³€ ìƒì„± ì‹¤íŒ¨]', error);
      setAnswer('ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  }
};
```

---

### 3. answerQuestionWithHistory í•¨ìˆ˜ ì¶”ê°€

**íŒŒì¼**: `projects/mkm/mkm-study20260120/src/utils/api.ts`

**ì¶”ê°€**:
```typescript
/**
 * ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì§ˆë¬¸ ë‹µë³€
 */
export async function answerQuestionWithHistory(
  question: string,
  currentState: Vector4D,
  history: Array<{role: string, content: string}> = []
): Promise<string> {
  // 4D ë²¡í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
  const context = `í˜„ì¬ ìƒíƒœ: S=${currentState.S.toFixed(2)}, L=${currentState.L.toFixed(2)}, K=${currentState.K.toFixed(2)}, M=${currentState.M.toFixed(2)}`;
  
  // ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
  const historyPrompt = history
    .map(msg => `${msg.role === 'user' ? 'ì‚¬ìš©ì' : 'AI'}: ${msg.content}`)
    .join('\n');
  
  const fullPrompt = history.length > 0
    ? `${context}\n\nì´ì „ ëŒ€í™”:\n${historyPrompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: ${question}\n\në‹µë³€:`
    : `${context}\n\nì‚¬ìš©ì ì§ˆë¬¸: ${question}\n\në‹µë³€:`;
  
  return await askGemma3(fullPrompt);
}
```

---

## ğŸ“‹ ì ìš© ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë¹ ë¥¸ ëª¨ë¸ ì ìš©

- [ ] VPSì— `llama3.2:3b` ëª¨ë¸ ì„¤ì¹˜ (`ollama pull llama3.2:3b`)
- [ ] `api.ts`ì—ì„œ ëª¨ë¸ ë³€ê²½ (`gemma3:4b` â†’ `llama3.2:3b`)
- [ ] í…ŒìŠ¤íŠ¸ (ì‘ë‹µ ì†ë„ í™•ì¸)

### ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€

- [ ] `MKMStudyApp.tsx`ì— `conversationHistory` ìƒíƒœ ì¶”ê°€
- [ ] `answerQuestionWithHistory` í•¨ìˆ˜ ì¶”ê°€
- [ ] `handleSpeechTranscript`ì—ì„œ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- [ ] í…ŒìŠ¤íŠ¸ (ëŒ€í™” ì§€ì†ì„± í™•ì¸)

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ì•ˆ

### "llama3.2:3b + ìˆ˜ë™ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬"

1. **ëª¨ë¸**: `llama3.2:3b` (ê°€ì¥ ë¹ ë¥´ê³  ëŒ€í™” ì§€ì†ì„± ìš°ìˆ˜)
2. **ì»¨í…ìŠ¤íŠ¸**: ìˆ˜ë™ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ì•ˆì •ì )
3. **ì €ì¥**: IndexedDBì— ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ì„¸ì…˜ ì¢…ë£Œ í›„ì—ë„ ìœ ì§€)

**ì˜ˆìƒ ì„±ëŠ¥**:
- ì²« ì‘ë‹µ: 200-500ms (í˜„ì¬ 500-1000ms ëŒ€ë¹„ 2-5ë°° ë¹ ë¦„)
- ëŒ€í™” ì§€ì†ì„±: 100% (ìˆ˜ë™ ê´€ë¦¬)
- ë©”ëª¨ë¦¬ ì‚¬ìš©: ~2GB (í˜„ì¬ì™€ ìœ ì‚¬)

---

## ğŸ”— ê´€ë ¨ íŒŒì¼

- **ë¡œì»¬ Gemma3 ì—”ì§„**: `tools/local_gemma3_engine.py`
- **ë¹ ë¥¸ ëª¨ë¸ ëª©ë¡**: `mcp-servers/ollama_insight_generator.py`
- **ìµœì í™”ëœ ëŒ€í™” ì—”ì§„**: `projects/mkm/mkm-study/utils/optimizedConversationEngine.ts`
- **ë¡œì»¬ ëª¨ë¸ ì „ëµ**: `docs/guides/ë¡œì»¬_ëª¨ë¸_ìš°ì„ _ì „ëµ_ê°€ì´ë“œ.md`

---

**ì‘ì„±ì¼**: 2026-01-20  
**ìƒíƒœ**: âœ… ë¶„ì„ ì™„ë£Œ, ì ìš© ê°€ì´ë“œ ì œê³µ

[Verified by Athena Auditor v1.2]

