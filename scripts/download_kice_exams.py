#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š ìˆ˜ëŠ¥/ëª¨ì˜ê³ ì‚¬ ê¸°ì¶œë¬¸ì œ ë‹¤ìš´ë¡œë”

í•œêµ­êµìœ¡ê³¼ì •í‰ê°€ì›(KICE)ì˜ ìˆ˜ëŠ¥ ë° ëª¨ì˜ê³ ì‚¬ ê¸°ì¶œë¬¸ì œë¥¼
ê³µê³µ ë°ì´í„°ë¡œ í™•ë³´í•˜ì—¬ 'ê¸°ì¤€ì (Ground Truth)'ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.

ì €ì‘ê¶Œ: êµ­ê°€ ì €ì‘ë¬¼ (ê³µê°œ, ì‚¬ìš© ììœ )
"""

import sys
import json
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import time
import re
from urllib.parse import urljoin, urlparse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# User-Agent ì„¤ì •
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/json, text/html, */*',
    'Accept-Language': 'ko-KR,ko;q=0.9',
}

# ìš”ì²­ ë”œë ˆì´
REQUEST_DELAY = 2.0  # 2ì´ˆ ê°„ê²©

# í‰ê°€ì› ê¸°ì¶œë¬¸ì œ URL (ì‹¤ì œ URL í™•ì¸ í•„ìš”)
KICE_BASE_URL = "https://www.kice.re.kr"
KICE_EXAM_URLS = {
    "ìˆ˜ëŠ¥": "https://www.kice.re.kr/boardCnts/list.do?boardID=1500230&m=040101&s=kice",  # ìˆ˜ëŠ¥ ê¸°ì¶œë¬¸ì œ
    "ëª¨ì˜ê³ ì‚¬": "https://www.kice.re.kr/boardCnts/list.do?boardID=1500231&m=040102&s=kice",  # ëª¨ì˜ê³ ì‚¬ ê¸°ì¶œë¬¸ì œ
    "í•™ë ¥í‰ê°€": "https://www.kice.re.kr/boardCnts/list.do?boardID=1500232&m=040103&s=kice",  # ì „êµ­ì—°í•©í•™ë ¥í‰ê°€
}

# ìµœê·¼ Në…„ì¹˜ ê¸°ì¶œë¬¸ì œ ìˆ˜ì§‘
YEARS_TO_COLLECT = 10  # ìµœê·¼ 10ë…„

def download_pdf(url: str, save_path: Path) -> bool:
    """PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        time.sleep(REQUEST_DELAY)
        response = requests.get(url, headers=HEADERS, timeout=30, stream=True)
        response.raise_for_status()
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        logger.info(f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path.name}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")
        return False

def extract_exam_metadata(html_content: str) -> List[Dict[str, Any]]:
    """ê¸°ì¶œë¬¸ì œ ëª©ë¡ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    from bs4 import BeautifulSoup
    
    soup = BeautifulSoup(html_content, 'html.parser')
    exams = []
    
    # í‰ê°€ì› ê²Œì‹œíŒ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹± (ì‹¤ì œ êµ¬ì¡° í™•ì¸ í•„ìš”)
    exam_items = soup.select('.exam-item, .board-item, .list-item')
    
    for item in exam_items:
        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = item.select_one('.title, h3, a')
            title = title_elem.get_text().strip() if title_elem else ""
            
            # ì—°ë„ ì¶”ì¶œ (ì˜ˆ: "2024í•™ë…„ë„")
            year_match = re.search(r'(\d{4})í•™ë…„ë„', title)
            year = year_match.group(1) if year_match else ""
            
            # PDF ë§í¬ ì¶”ì¶œ
            link_elem = item.select_one('a[href*=".pdf"], a[href*="download"]')
            pdf_url = ""
            if link_elem:
                href = link_elem.get('href', '')
                if href.startswith('http'):
                    pdf_url = href
                elif href.startswith('/'):
                    pdf_url = urljoin(KICE_BASE_URL, href)
                else:
                    pdf_url = urljoin(KICE_BASE_URL, href)
            
            # ê³¼ëª© ì¶”ì¶œ (ìˆ˜í•™, ì˜ì–´ ë“±)
            subject = ""
            if "ìˆ˜í•™" in title or "math" in title.lower():
                subject = "math"
            elif "ì˜ì–´" in title or "english" in title.lower():
                subject = "english"
            
            if title and year:
                exams.append({
                    "title": title,
                    "year": year,
                    "subject": subject,
                    "pdf_url": pdf_url,
                    "exam_type": "ìˆ˜ëŠ¥" if "ìˆ˜ëŠ¥" in title else "ëª¨ì˜ê³ ì‚¬"
                })
                
        except Exception as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            continue
    
    return exams

def scrape_kice_exams(exam_type: str = "ìˆ˜ëŠ¥") -> List[Dict[str, Any]]:
    """
    í‰ê°€ì› ê¸°ì¶œë¬¸ì œ ëª©ë¡ ìˆ˜ì§‘
    
    Args:
        exam_type: ì‹œí—˜ ìœ í˜• ("ìˆ˜ëŠ¥", "ëª¨ì˜ê³ ì‚¬", "í•™ë ¥í‰ê°€")
    
    Returns:
        ê¸°ì¶œë¬¸ì œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"í‰ê°€ì› ê¸°ì¶œë¬¸ì œ ìˆ˜ì§‘ ì‹œì‘: {exam_type}")
    
    if exam_type not in KICE_EXAM_URLS:
        logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‹œí—˜ ìœ í˜•: {exam_type}")
        return []
    
    url = KICE_EXAM_URLS[exam_type]
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        exams = extract_exam_metadata(response.text)
        
        # ìµœê·¼ Në…„ì¹˜ë§Œ í•„í„°ë§
        current_year = datetime.now().year
        filtered_exams = [
            exam for exam in exams
            if exam.get("year") and int(exam.get("year", 0)) >= (current_year - YEARS_TO_COLLECT)
        ]
        
        logger.info(f"âœ… {len(filtered_exams)}ê°œ ê¸°ì¶œë¬¸ì œ ë°œê²¬")
        return filtered_exams
        
    except Exception as e:
        logger.error(f"âŒ ê¸°ì¶œë¬¸ì œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def download_exam_pdfs(exams: List[Dict[str, Any]], output_dir: Path):
    """ê¸°ì¶œë¬¸ì œ PDF ë‹¤ìš´ë¡œë“œ"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded = 0
    failed = 0
    
    for exam in exams:
        if not exam.get("pdf_url"):
            continue
        
        # íŒŒì¼ëª… ìƒì„±
        filename = f"{exam['year']}_{exam['exam_type']}_{exam['subject']}_{exam['title'][:20]}.pdf"
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)  # íŒŒì¼ëª…ì— ì‚¬ìš© ë¶ˆê°€ ë¬¸ì ì œê±°
        
        save_path = output_dir / filename
        
        if save_path.exists():
            logger.info(f"â­ï¸ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨: {filename}")
            continue
        
        if download_pdf(exam["pdf_url"], save_path):
            downloaded += 1
        else:
            failed += 1
    
    logger.info(f"\në‹¤ìš´ë¡œë“œ ì™„ë£Œ: ì„±ê³µ {downloaded}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")

def convert_pdf_to_text(pdf_path: Path) -> str:
    """PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (OCR ë˜ëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œ)"""
    try:
        # PyPDF2 ë˜ëŠ” pdfplumber ì‚¬ìš©
        try:
            import pdfplumber
            with pdfplumber.open(pdf_path) as pdf:
                text = ""
                for page in pdf.pages:
                    text += page.extract_text() or ""
                return text
        except ImportError:
            try:
                import PyPDF2
                with open(pdf_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() or ""
                    return text
            except ImportError:
                logger.warning("PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install pdfplumber ë˜ëŠ” PyPDF2 í•„ìš”")
                return ""
    except Exception as e:
        logger.error(f"PDF ë³€í™˜ ì‹¤íŒ¨ ({pdf_path}): {e}")
        return ""

def process_exam_pdfs(pdf_dir: Path, output_dir: Path):
    """ê¸°ì¶œë¬¸ì œ PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    pdf_files = list(pdf_dir.glob("*.pdf"))
    logger.info(f"PDF ë³€í™˜ ì‹œì‘: {len(pdf_files)}ê°œ íŒŒì¼")
    
    converted = 0
    
    for pdf_file in pdf_files:
        try:
            text = convert_pdf_to_text(pdf_file)
            
            if text:
                # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                text_file = output_dir / f"{pdf_file.stem}.txt"
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(text)
                
                converted += 1
                logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {pdf_file.name}")
            else:
                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_file.name}")
                
        except Exception as e:
            logger.error(f"âŒ ë³€í™˜ ì˜¤ë¥˜ ({pdf_file}): {e}")
    
    logger.info(f"\në³€í™˜ ì™„ë£Œ: {converted}/{len(pdf_files)}ê°œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("ìˆ˜ëŠ¥/ëª¨ì˜ê³ ì‚¬ ê¸°ì¶œë¬¸ì œ ë‹¤ìš´ë¡œë”")
    logger.info("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_base = Path("learning-content/kice-exams")
    pdf_dir = output_base / "pdfs"
    text_dir = output_base / "texts"
    metadata_dir = output_base / "metadata"
    
    # ê¸°ì¶œë¬¸ì œ ìˆ˜ì§‘
    all_exams = []
    
    for exam_type in ["ìˆ˜ëŠ¥", "ëª¨ì˜ê³ ì‚¬"]:
        exams = scrape_kice_exams(exam_type)
        all_exams.extend(exams)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_dir.mkdir(parents=True, exist_ok=True)
    metadata_path = metadata_dir / "exam_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(all_exams, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(all_exams)}ê°œ")
    
    # PDF ë‹¤ìš´ë¡œë“œ (ì„ íƒì )
    user_input = input("\nPDF ë‹¤ìš´ë¡œë“œë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if user_input == 'y':
        download_exam_pdfs(all_exams, pdf_dir)
        
        # PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì„ íƒì )
        user_input2 = input("\nPDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if user_input2 == 'y':
            process_exam_pdfs(pdf_dir, text_dir)
    
    logger.info("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
    logger.info("1. ê¸°ì¶œë¬¸ì œ ë©”íƒ€ë°ì´í„° í™•ì¸: learning-content/kice-exams/metadata/exam_metadata.json")
    logger.info("2. Athena Generator êµ¬ì¶•: ê¸°ì¶œë¬¸ì œë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œ ìƒì„± í”„ë¡¬í”„íŠ¸ ì‘ì„±")
    logger.info("3. í•©ì„± ë¬¸ì œ ìƒì„±: ì»¤ë¦¬í˜ëŸ¼ ë§µ + ê¸°ì¶œë¬¸ì œ ë¶„ì„ â†’ ë§ì¶¤í˜• ë¬¸ì œ ìƒì„±")

if __name__ == "__main__":
    main()

