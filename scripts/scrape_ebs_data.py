#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EBS ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

EBS ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìˆ˜í•™/ì˜ì–´ êµê³¼ê³¼ì • ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬
í•™ìŠµ ì •ë³´ ì‹œìŠ¤í…œì— ì €ì¥í•©ë‹ˆë‹¤.

ì£¼ì˜: robots.txt í™•ì¸ ë° ì €ì‘ê¶Œ ì¤€ìˆ˜ í•„ìˆ˜
"""

import sys
import json
import time
import requests
from bs4 import BeautifulSoup
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import hashlib
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# API ì„¤ì •
API_BASE = "http://148.230.97.246:8003"

# User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# EBS ì‚¬ì´íŠ¸ URL (ì˜ˆì‹œ - ì‹¤ì œ URL êµ¬ì¡° í™•ì¸ í•„ìš”)
EBS_URLS = {
    "math_middle": "https://mid.ebs.co.kr/ebs/mid/midMain",  # ì¤‘í•™ ìˆ˜í•™
    "math_high": "https://www.ebsi.co.kr/ebs/lms/lmsx/retrieveSbjtAtclList.ebs?sbjtId=MATH",  # ê³ êµ ìˆ˜í•™
    "english_middle": "https://mid.ebs.co.kr/ebs/mid/midMain",  # ì¤‘í•™ ì˜ì–´
    "english_high": "https://www.ebsi.co.kr/ebs/lms/lmsx/retrieveSbjtAtclList.ebs?sbjtId=ENG",  # ê³ êµ ì˜ì–´
}

# ìš”ì²­ ë”œë ˆì´ (ì´ˆë‹¹ ìš”ì²­ ì œí•œ ì¤€ìˆ˜)
REQUEST_DELAY = 2.0  # 2ì´ˆ ê°„ê²©

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ (HTML íƒœê·¸, ê³µë°± ì œê±°)"""
    if not text:
        return ""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()

def scrape_ebs_course_list(url: str, subject: str, grade: str) -> List[Dict[str, Any]]:
    """
    EBS ê°•ì¢Œ ëª©ë¡ ìŠ¤í¬ë˜í•‘
    
    Args:
        url: EBS ê°•ì¢Œ ëª©ë¡ URL
        subject: ê³¼ëª© (math ë˜ëŠ” english)
        grade: í•™ë…„ (ì¤‘1, ì¤‘2, ì¤‘3, ê³ 1, ê³ 2, ê³ 3)
    
    Returns:
        ê°•ì¢Œ ëª©ë¡ (ì œëª©, URL, ì„¤ëª… ë“±)
    """
    logger.info(f"EBS ê°•ì¢Œ ëª©ë¡ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        courses = []
        
        # EBS ê°•ì¢Œ ë¦¬ìŠ¤íŠ¸ ì„ íƒì (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        # ì˜ˆì‹œ: .course_list, .lecture_item ë“±
        course_elements = soup.select('.course_list .item, .lecture_list .item, .sbjt_list .item')
        
        if not course_elements:
            # ëŒ€ì²´ ì„ íƒì ì‹œë„
            course_elements = soup.select('a[href*="course"], a[href*="lecture"], .title a')
        
        for element in course_elements:
            try:
                # ì œëª© ì¶”ì¶œ
                title_elem = element.select_one('.title, h3, h4, a')
                if not title_elem:
                    continue
                
                title = clean_text(title_elem.get_text())
                if not title:
                    continue
                
                # URL ì¶”ì¶œ
                link_elem = element.select_one('a')
                course_url = ""
                if link_elem and link_elem.get('href'):
                    href = link_elem.get('href')
                    if href.startswith('http'):
                        course_url = href
                    elif href.startswith('/'):
                        course_url = f"https://www.ebsi.co.kr{href}"
                    else:
                        course_url = f"{url}/{href}"
                
                # ì„¤ëª… ì¶”ì¶œ
                desc_elem = element.select_one('.desc, .description, p')
                description = clean_text(desc_elem.get_text()) if desc_elem else ""
                
                # ê°•ì‚¬ëª… ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
                teacher_elem = element.select_one('.teacher, .instructor, .author')
                teacher = clean_text(teacher_elem.get_text()) if teacher_elem else ""
                
                courses.append({
                    "title": title,
                    "url": course_url,
                    "description": description,
                    "teacher": teacher,
                    "subject": subject,
                    "grade": grade
                })
                
            except Exception as e:
                logger.warning(f"ê°•ì¢Œ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… {len(courses)}ê°œ ê°•ì¢Œ ìˆ˜ì§‘ ì™„ë£Œ")
        return courses
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ ìš”ì²­ ì‹¤íŒ¨ ({url}): {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url}): {e}")
        return []

def scrape_ebs_course_detail(course_url: str) -> Dict[str, Any]:
    """
    EBS ê°•ì¢Œ ìƒì„¸ ì •ë³´ ìŠ¤í¬ë˜í•‘
    
    Args:
        course_url: ê°•ì¢Œ ìƒì„¸ í˜ì´ì§€ URL
    
    Returns:
        ê°•ì¢Œ ìƒì„¸ ì •ë³´ (í•™ìŠµ ëª©í‘œ, ë‚´ìš© ë“±)
    """
    if not course_url:
        return {}
    
    logger.info(f"ê°•ì¢Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘: {course_url}")
    
    try:
        time.sleep(REQUEST_DELAY)  # ìš”ì²­ ë”œë ˆì´
        
        response = requests.get(course_url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content_elem = soup.select_one('.content, .main-content, .article-body, #content')
        content = clean_text(content_elem.get_text()) if content_elem else ""
        
        # í•™ìŠµ ëª©í‘œ ì¶”ì¶œ
        objective_elem = soup.select_one('.objective, .learning-goal, .goal')
        objective = clean_text(objective_elem.get_text()) if objective_elem else ""
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        keyword_elems = soup.select('.keyword, .tag, .label')
        for elem in keyword_elems:
            keyword = clean_text(elem.get_text())
            if keyword:
                keywords.append(keyword)
        
        return {
            "content": content,
            "objective": objective,
            "keywords": keywords
        }
        
    except Exception as e:
        logger.warning(f"ê°•ì¢Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({course_url}): {e}")
        return {}

def scrape_aihub_qa_data() -> List[Dict[str, Any]]:
    """
    AI Hubì—ì„œ êµê³¼ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ë°ì´í„° ìˆ˜ì§‘
    
    ì°¸ê³ : AI HubëŠ” íšŒì›ê°€ì… í›„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ í•„ìš”
    """
    logger.info("AI Hub ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”)")
    
    # AI Hub ë°ì´í„°ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í›„ JSON íŒŒì¼ë¡œ ì €ì¥ í•„ìš”
    # ì—¬ê¸°ì„œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œí•˜ëŠ” ì˜ˆì‹œë§Œ ì œê³µ
    
    aihub_data_path = Path("learning-content/aihub_qa_data.json")
    
    if not aihub_data_path.exists():
        logger.warning("AI Hub ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        logger.info("1. https://aihub.or.kr ì ‘ì†")
        logger.info("2. 'ì´ˆì¤‘ê³  í•™ìƒ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°' ê²€ìƒ‰")
        logger.info("3. ë°ì´í„° ë‹¤ìš´ë¡œë“œ í›„ learning-content/aihub_qa_data.jsonìœ¼ë¡œ ì €ì¥")
        return []
    
    try:
        with open(aihub_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"âœ… AI Hub ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ í•­ëª©")
        return data
        
    except Exception as e:
        logger.error(f"âŒ AI Hub ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def scrape_public_data_portal(api_key: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ êµìœ¡ê³¼ì • ë°ì´í„° ìˆ˜ì§‘
    
    Args:
        api_key: ê³µê³µë°ì´í„°í¬í„¸ API í‚¤ (ì„ íƒì )
    """
    logger.info("ê³µê³µë°ì´í„°í¬í„¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    if not api_key:
        logger.warning("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”:")
        logger.info("1. https://www.data.go.kr ì ‘ì†")
        logger.info("2. 'êµìœ¡ê³¼ì •' ê²€ìƒ‰")
        logger.info("3. ì›í•˜ëŠ” ë°ì´í„°ì…‹ ì„ íƒ í›„ API í‚¤ ë°œê¸‰")
        return []
    
    # ê³µê³µë°ì´í„°í¬í„¸ API í˜¸ì¶œ ì˜ˆì‹œ
    # ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ëŠ” ë°ì´í„°ì…‹ë§ˆë‹¤ ë‹¤ë¦„
    try:
        # ì˜ˆì‹œ: êµìœ¡ê³¼ì • ì •ë³´ API
        api_url = f"https://www.data.go.kr/api/êµìœ¡ê³¼ì •ì •ë³´?serviceKey={api_key}"
        response = requests.get(api_url, timeout=15)
        response.raise_for_status()
        
        # XML ë˜ëŠ” JSON íŒŒì‹± (ì‹¤ì œ ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •)
        data = response.json() if response.headers.get('Content-Type', '').startswith('application/json') else {}
        
        logger.info(f"âœ… ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(data.get('items', []))}ê°œ í•­ëª©")
        return data.get('items', [])
        
    except Exception as e:
        logger.error(f"âŒ ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def save_to_api(content_data: Dict[str, Any]) -> bool:
    """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ VPS APIì— ì €ì¥"""
    try:
        response = requests.post(
            f"{API_BASE}/api/v1/learning/store",
            json=content_data,
            timeout=10
        )
        
        if response.status_code == 200:
            logger.info(f"âœ… API ì €ì¥ ì„±ê³µ: {content_data.get('topic', 'Unknown')}")
            return True
        else:
            logger.warning(f"âš ï¸ API ì €ì¥ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ API ì €ì¥ ì˜¤ë¥˜: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("EBS ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 60)
    
    # ìˆ˜ì§‘í•  ë°ì´í„° ëª©ë¡
    collection_plan = [
        {"subject": "math", "grade": "ì¤‘1", "url": EBS_URLS["math_middle"]},
        {"subject": "math", "grade": "ì¤‘2", "url": EBS_URLS["math_middle"]},
        {"subject": "math", "grade": "ì¤‘3", "url": EBS_URLS["math_middle"]},
        {"subject": "math", "grade": "ê³ 1", "url": EBS_URLS["math_high"]},
        {"subject": "math", "grade": "ê³ 2", "url": EBS_URLS["math_high"]},
        {"subject": "english", "grade": "ì¤‘1", "url": EBS_URLS["english_middle"]},
        {"subject": "english", "grade": "ì¤‘2", "url": EBS_URLS["english_middle"]},
        {"subject": "english", "grade": "ì¤‘3", "url": EBS_URLS["english_middle"]},
        {"subject": "english", "grade": "ê³ 1", "url": EBS_URLS["english_high"]},
        {"subject": "english", "grade": "ê³ 2", "url": EBS_URLS["english_high"]},
    ]
    
    total_collected = 0
    total_saved = 0
    
    # EBS ê°•ì¢Œ ëª©ë¡ ìˆ˜ì§‘
    for plan in collection_plan:
        logger.info(f"\nğŸ“š {plan['grade']} {plan['subject']} ìˆ˜ì§‘ ì‹œì‘...")
        
        courses = scrape_ebs_course_list(
            plan['url'],
            plan['subject'],
            plan['grade']
        )
        
        total_collected += len(courses)
        
        # ê° ê°•ì¢Œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ë° ì €ì¥
        for course in courses:
            # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            detail = scrape_ebs_course_detail(course.get('url', ''))
            
            # í•™ìŠµ ì½˜í…ì¸  ë°ì´í„° êµ¬ì„±
            content_data = {
                "subject": plan['subject'],
                "topic": course['title'],
                "content": detail.get('content', course.get('description', '')),
                "difficulty": "medium",  # ê¸°ë³¸ê°’
                "ebsCurriculum": f"EBS {plan['grade']} {plan['subject']}",
                "keyTopics": detail.get('keywords', []),
                "grade": plan['grade'],
                "teacher": course.get('teacher', ''),
                "objective": detail.get('objective', ''),
                "url": course.get('url', ''),
                "createdAt": datetime.now().isoformat(),
                "updatedAt": datetime.now().isoformat()
            }
            
            # APIì— ì €ì¥
            if save_to_api(content_data):
                total_saved += 1
            
            time.sleep(REQUEST_DELAY)  # ìš”ì²­ ë”œë ˆì´
    
    # AI Hub ë°ì´í„° ìˆ˜ì§‘ (ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš”)
    logger.info("\nğŸ¤– AI Hub ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    aihub_data = scrape_aihub_qa_data()
    
    if aihub_data:
        for item in aihub_data:
            content_data = {
                "subject": item.get('subject', 'general'),
                "topic": item.get('question', ''),
                "content": item.get('answer', ''),
                "difficulty": item.get('difficulty', 'medium'),
                "ebsCurriculum": "AI Hub",
                "keyTopics": item.get('keywords', []),
                "createdAt": datetime.now().isoformat(),
                "updatedAt": datetime.now().isoformat()
            }
            
            if save_to_api(content_data):
                total_saved += 1
    
    # ê³µê³µë°ì´í„° ìˆ˜ì§‘ (API í‚¤ í•„ìš”)
    logger.info("\nğŸ“Š ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    public_data = scrape_public_data_portal(api_key=None)  # API í‚¤ ì„¤ì • í•„ìš”
    
    if public_data:
        for item in public_data:
            content_data = {
                "subject": item.get('subject', 'general'),
                "topic": item.get('title', ''),
                "content": item.get('content', ''),
                "difficulty": "medium",
                "ebsCurriculum": "ê³µê³µë°ì´í„°í¬í„¸",
                "keyTopics": item.get('keywords', []),
                "createdAt": datetime.now().isoformat(),
                "updatedAt": datetime.now().isoformat()
            }
            
            if save_to_api(content_data):
                total_saved += 1
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "=" * 60)
    logger.info("ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½")
    logger.info("=" * 60)
    logger.info(f"ì´ ìˆ˜ì§‘: {total_collected}ê°œ ê°•ì¢Œ")
    logger.info(f"ì´ ì €ì¥: {total_saved}ê°œ í•­ëª©")
    logger.info("=" * 60)

if __name__ == "__main__":
    main()

